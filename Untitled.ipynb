{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99cb4528-00a2-40b3-8b45-4eba3561adc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEDI shots inside ROI: 2\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd, h5py, numpy as np, pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "ROI = \"field_boundary.geojson\"     # your small-area geojson\n",
    "GEDI = \"Field_Boundary\\GEDI04_A_2019107224731_O01958_03_T02638_02_002_02_V002.h5\"  # any of your 29 files\n",
    "\n",
    "roi_ll = gpd.read_file(ROI).to_crs(4326)\n",
    "\n",
    "def read_gedi_good(h5_path):\n",
    "    pts=[]\n",
    "    with h5py.File(h5_path,\"r\") as f:\n",
    "        for b in [k for k in f.keys() if k.startswith(\"BEAM\")]:\n",
    "            if f.get(f\"{b}/agbd\") is None: continue\n",
    "            lat=f[f\"{b}/lat_lowestmode\"][:]; lon=f[f\"{b}/lon_lowestmode\"][:]\n",
    "            ag = f[f\"{b}/agbd\"][:]; q  = f[f\"{b}/l4_quality_flag\"][:]\n",
    "            ok = np.isfinite(lat)&np.isfinite(lon)&np.isfinite(ag)&(q==1)\n",
    "            pts.append(pd.DataFrame({\"lon\":lon[ok],\"lat\":lat[ok],\"agbd\":ag[ok]}))\n",
    "    df = pd.concat(pts, ignore_index=True) if pts else pd.DataFrame(columns=[\"lon\",\"lat\",\"agbd\"])\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=[Point(xy) for xy in zip(df.lon, df.lat)], crs=4326)\n",
    "    return gdf\n",
    "\n",
    "gedi = read_gedi_good(GEDI)\n",
    "\n",
    "# keep GEDI shots inside ROI\n",
    "gedi_in = gpd.sjoin(gedi, roi_ll.to_crs(4326), predicate=\"within\").drop(columns=[\"index_right\"])\n",
    "print(\"GEDI shots inside ROI:\", len(gedi_in))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4ded638-0a8b-4a44-9381-a4fff3a40b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Masfiq\\AppData\\Local\\Temp\\ipykernel_53792\\3131851263.py:14: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  if box(*src.bounds).intersects(roi_ll.to_crs(src.crs).unary_union):\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'xr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bit \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     45\u001b[0m     clear \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m ((qa_u \u001b[38;5;241m>>\u001b[39m bit) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 46\u001b[0m clear \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241m.\u001b[39mDataArray(clear, coords\u001b[38;5;241m=\u001b[39mqa\u001b[38;5;241m.\u001b[39mcoords, dims\u001b[38;5;241m=\u001b[39mqa\u001b[38;5;241m.\u001b[39mdims)\n\u001b[0;32m     48\u001b[0m nir  \u001b[38;5;241m=\u001b[39m nir\u001b[38;5;241m.\u001b[39mwhere(clear); red \u001b[38;5;241m=\u001b[39m red\u001b[38;5;241m.\u001b[39mwhere(clear); blue \u001b[38;5;241m=\u001b[39m blue\u001b[38;5;241m.\u001b[39mwhere(clear)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# indices\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xr' is not defined"
     ]
    }
   ],
   "source": [
    "import rioxarray as rxr, rasterio, re, os\n",
    "from shapely.geometry import box\n",
    "\n",
    "BASE = \"hls_downloads\"  # where you saved per-granule folders\n",
    "\n",
    "# pick the first granule whose bounds intersect ROI\n",
    "granule_dir = None\n",
    "for gname in os.listdir(BASE):\n",
    "    gdir = os.path.join(BASE, gname)\n",
    "    if not os.path.isdir(gdir): continue\n",
    "    b04 = next((os.path.join(gdir,f) for f in os.listdir(gdir) if f.endswith(\".B04.tif\")), None)\n",
    "    if not b04: continue\n",
    "    with rasterio.open(b04) as src:\n",
    "        if box(*src.bounds).intersects(roi_ll.to_crs(src.crs).unary_union):\n",
    "            granule_dir = gdir; hls_crs = src.crs\n",
    "            break\n",
    "assert granule_dir, \"No downloaded HLS granule overlaps the ROI.\"\n",
    "\n",
    "def pick(gdir, tag):\n",
    "    for f in os.listdir(gdir):\n",
    "        if re.search(rf\"\\.{tag}\\.tif$\", f): return os.path.join(gdir,f)\n",
    "    return None\n",
    "\n",
    "is_s30 = \"HLS.S30\" in granule_dir or \"HLSS30\" in granule_dir\n",
    "nir_tag = \"B8A\" if is_s30 else \"B05\"\n",
    "\n",
    "nir  = rxr.open_rasterio(pick(granule_dir, nir_tag), chunks={\"x\":512,\"y\":512}).squeeze(\"band\",drop=True).astype(\"float32\")*1e-4\n",
    "red  = rxr.open_rasterio(pick(granule_dir, \"B04\"),     chunks={\"x\":512,\"y\":512}).squeeze(\"band\",drop=True).astype(\"float32\")*1e-4\n",
    "blue = rxr.open_rasterio(pick(granule_dir, \"B02\"),     chunks={\"x\":512,\"y\":512}).squeeze(\"band\",drop=True).astype(\"float32\")*1e-4\n",
    "qa   = rxr.open_rasterio(pick(granule_dir, \"Fmask\"),   chunks={\"x\":512,\"y\":512}).squeeze(\"band\",drop=True)\n",
    "\n",
    "roi_utm = roi_ll.to_crs(nir.rio.crs)\n",
    "geom = list(roi_utm.geometry)\n",
    "\n",
    "nir  = nir.rio.clip(geom, roi_utm.crs, drop=True, all_touched=True, from_disk=True)\n",
    "red  = red.rio.clip(geom, roi_utm.crs, drop=True, all_touched=True, from_disk=True)\n",
    "blue = blue.rio.clip(geom, roi_utm.crs, drop=True, all_touched=True, from_disk=True)\n",
    "qa   = qa.rio.clip(geom,  roi_utm.crs, drop=True, all_touched=True, from_disk=True)\n",
    "\n",
    "# simple clear mask using Fmask bits 1..5 (cloud/shadow/snow/water) == 0\n",
    "import numpy as np\n",
    "qa_u = qa.fillna(0).astype(\"uint16\").values\n",
    "clear = np.ones_like(qa_u, dtype=bool)\n",
    "for bit in (1,2,3,4,5):\n",
    "    clear &= ((qa_u >> bit) & 1) == 0\n",
    "clear = xr.DataArray(clear, coords=qa.coords, dims=qa.dims)\n",
    "\n",
    "nir  = nir.where(clear); red = red.where(clear); blue = blue.where(clear)\n",
    "\n",
    "# indices\n",
    "ndvi = (nir - red) / (nir + red + 1e-6)\n",
    "evi  = 2.5 * (nir - red) / (nir + 6*red - 7.5*blue + 1.0 + 1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43f03524-8d85-4e32-ab1e-e25746b1967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray as rxr, rasterio, re, os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from shapely.geometry import box\n",
    "\n",
    "BASE = \"hls_downloads\"  # where you saved per-granule folders\n",
    "\n",
    "# pick the first granule whose bounds intersect ROI\n",
    "granule_dir = None\n",
    "for gname in os.listdir(BASE):\n",
    "    gdir = os.path.join(BASE, gname)\n",
    "    if not os.path.isdir(gdir): \n",
    "        continue\n",
    "    b04 = next((os.path.join(gdir, f) for f in os.listdir(gdir) if f.endswith(\".B04.tif\")), None)\n",
    "    if not b04:\n",
    "        continue\n",
    "    with rasterio.open(b04) as src:\n",
    "        # use union_all() instead of deprecated unary_union\n",
    "        if box(*src.bounds).intersects(roi_ll.to_crs(src.crs).union_all()):\n",
    "            granule_dir = gdir\n",
    "            hls_crs = src.crs\n",
    "            break\n",
    "\n",
    "assert granule_dir, \"No downloaded HLS granule overlaps the ROI.\"\n",
    "\n",
    "def pick(gdir, tag):\n",
    "    for f in os.listdir(gdir):\n",
    "        if re.search(rf\"\\.{tag}\\.tif$\", f):\n",
    "            return os.path.join(gdir, f)\n",
    "    return None\n",
    "\n",
    "is_s30  = (\"HLS.S30\" in granule_dir) or (\"HLSS30\" in granule_dir)\n",
    "nir_tag = \"B8A\" if is_s30 else \"B05\"\n",
    "\n",
    "nir  = rxr.open_rasterio(pick(granule_dir, nir_tag), chunks={\"x\":512,\"y\":512}).squeeze(\"band\", drop=True).astype(\"float32\") * 1e-4\n",
    "red  = rxr.open_rasterio(pick(granule_dir, \"B04\"),   chunks={\"x\":512,\"y\":512}).squeeze(\"band\", drop=True).astype(\"float32\") * 1e-4\n",
    "blue = rxr.open_rasterio(pick(granule_dir, \"B02\"),   chunks={\"x\":512,\"y\":512}).squeeze(\"band\", drop=True).astype(\"float32\") * 1e-4\n",
    "qa   = rxr.open_rasterio(pick(granule_dir, \"Fmask\"), chunks={\"x\":512,\"y\":512}).squeeze(\"band\", drop=True)\n",
    "\n",
    "roi_utm = roi_ll.to_crs(nir.rio.crs)\n",
    "geom = list(roi_utm.geometry)\n",
    "\n",
    "nir  = nir.rio.clip(geom,  roi_utm.crs, drop=True, all_touched=True, from_disk=True)\n",
    "red  = red.rio.clip(geom,  roi_utm.crs, drop=True, all_touched=True, from_disk=True)\n",
    "blue = blue.rio.clip(geom, roi_utm.crs, drop=True, all_touched=True, from_disk=True)\n",
    "qa   = qa.rio.clip(geom,   roi_utm.crs, drop=True, all_touched=True, from_disk=True)\n",
    "\n",
    "# clear mask from Fmask bits 1..5 == 0 (cloud/shadow/snow/water off)\n",
    "qa_u = qa.fillna(0).astype(\"uint16\").values\n",
    "clear_np = np.ones_like(qa_u, dtype=bool)\n",
    "for bit in (1,2,3,4,5):\n",
    "    clear_np &= ((qa_u >> bit) & 1) == 0\n",
    "clear = xr.DataArray(clear_np, coords=qa.coords, dims=qa.dims)\n",
    "\n",
    "nir  = nir.where(clear)\n",
    "red  = red.where(clear)\n",
    "blue = blue.where(clear)\n",
    "\n",
    "# indices\n",
    "ndvi = (nir - red) / (nir + red + 1e-6)\n",
    "evi  = 2.5 * (nir - red) / (nir + 6*red - 7.5*blue + 1.0 + 1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a8579a5-2a98-426d-b81d-b5823477be7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RasterArray' object has no attribute 'source'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m xs \u001b[38;5;241m=\u001b[39m gedi_utm\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     19\u001b[0m ys \u001b[38;5;241m=\u001b[39m gedi_utm\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m---> 21\u001b[0m red_v  \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromiter\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_da\u001b[49m\u001b[43m(\u001b[49m\u001b[43mred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m nir_v  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfromiter(sample_da(nir,  xs, ys), \u001b[38;5;28mfloat\u001b[39m, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(xs))\n\u001b[0;32m     23\u001b[0m blue_v \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfromiter(sample_da(blue, xs, ys), \u001b[38;5;28mfloat\u001b[39m, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(xs))\n",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m, in \u001b[0;36msample_da\u001b[1;34m(da, xs, ys)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msample_da\u001b[39m(da, xs, ys):\n\u001b[1;32m---> 13\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[43mda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource\u001b[49m   \u001b[38;5;66;03m# underlying raster path\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m rasterio\u001b[38;5;241m.\u001b[39mopen(path) \u001b[38;5;28;01mas\u001b[39;00m src:\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m src\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(xs, ys))):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RasterArray' object has no attribute 'source'"
     ]
    }
   ],
   "source": [
    "import xarray as xr, numpy as np, rasterio\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import math\n",
    "\n",
    "# GEDI → HLS CRS and keep only points inside this clip’s bbox\n",
    "gedi_utm = gedi_in.to_crs(nir.rio.crs)\n",
    "minx, miny, maxx, maxy = nir.rio.bounds()\n",
    "gedi_utm = gedi_utm.cx[minx:maxx, miny:maxy]\n",
    "\n",
    "# fast sampling via rasterio\n",
    "def sample_da(da, xs, ys):\n",
    "    path = da.rio.source   # underlying raster path\n",
    "    with rasterio.open(path) as src:\n",
    "        for v in src.sample(list(zip(xs, ys))):\n",
    "            yield float(v)\n",
    "\n",
    "xs = gedi_utm.geometry.x.values\n",
    "ys = gedi_utm.geometry.y.values\n",
    "\n",
    "red_v  = np.fromiter(sample_da(red,  xs, ys), float, count=len(xs))\n",
    "nir_v  = np.fromiter(sample_da(nir,  xs, ys), float, count=len(xs))\n",
    "blue_v = np.fromiter(sample_da(blue, xs, ys), float, count=len(xs))\n",
    "ndvi_v = (nir_v - red_v)/(nir_v + red_v + 1e-6)\n",
    "evi_v  = 2.5*(nir_v - red_v)/(nir_v + 6*red_v - 7.5*blue_v + 1.0 + 1e-6)\n",
    "\n",
    "df = pd.DataFrame({\"agbd\": gedi_utm[\"agbd\"].values,\n",
    "                   \"red\": red_v, \"nir\": nir_v, \"blue\": blue_v,\n",
    "                   \"ndvi\": ndvi_v, \"evi\": evi_v}).replace([np.inf,-np.inf], np.nan).dropna()\n",
    "\n",
    "X = df[[\"red\",\"nir\",\"blue\",\"ndvi\",\"evi\"]].values\n",
    "y = df[\"agbd\"].values\n",
    "model = RidgeCV(alphas=(0.1,1,10,100)).fit(X, y)\n",
    "pred = model.predict(X)\n",
    "rmse = math.sqrt(mean_squared_error(y, pred)); r2 = r2_score(y, pred)\n",
    "print(f\"Baseline — RMSE={rmse:.1f} Mg/ha, R²={r2:.3f}, n={len(y)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cddf552-19ec-4557-8819-59c3e560ae0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline — RMSE=2.3 Mg/ha, R²=0.000, n=2\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "# Keep only GEDI points inside the clipped raster bounds\n",
    "minx, miny, maxx, maxy = nir.rio.bounds()\n",
    "pts = gedi_utm[(gedi_utm.geometry.x >= minx) & (gedi_utm.geometry.x <= maxx) &\n",
    "               (gedi_utm.geometry.y >= miny) & (gedi_utm.geometry.y <= maxy)]\n",
    "\n",
    "# Vectorized nearest-neighbour sampling from the DataArrays\n",
    "xs = xr.DataArray(pts.geometry.x.to_numpy(), dims=\"p\")\n",
    "ys = xr.DataArray(pts.geometry.y.to_numpy(), dims=\"p\")\n",
    "\n",
    "red_v  = red.sel(x=xs,  y=ys,  method=\"nearest\").to_numpy()\n",
    "nir_v  = nir.sel(x=xs,  y=ys,  method=\"nearest\").to_numpy()\n",
    "blue_v = blue.sel(x=xs, y=ys, method=\"nearest\").to_numpy()\n",
    "\n",
    "# Indices from sampled values (arrays are already scaled & masked)\n",
    "ndvi_v = (nir_v - red_v) / (nir_v + red_v + 1e-6)\n",
    "evi_v  = 2.5 * (nir_v - red_v) / (nir_v + 6*red_v - 7.5*blue_v + 1.0 + 1e-6)\n",
    "\n",
    "# Build dataframe, drop NaNs (masked/edge pixels), train RidgeCV\n",
    "df = pd.DataFrame({\n",
    "    \"agbd\": pts[\"agbd\"].to_numpy(),\n",
    "    \"red\": red_v, \"nir\": nir_v, \"blue\": blue_v,\n",
    "    \"ndvi\": ndvi_v, \"evi\": evi_v\n",
    "}).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "X = df[[\"red\",\"nir\",\"blue\",\"ndvi\",\"evi\"]].to_numpy()\n",
    "y = df[\"agbd\"].to_numpy()\n",
    "\n",
    "model = RidgeCV(alphas=(0.1, 1, 10, 100)).fit(X, y)\n",
    "pred  = model.predict(X)\n",
    "rmse  = math.sqrt(mean_squared_error(y, pred))\n",
    "r2    = r2_score(y, pred)\n",
    "print(f\"Baseline — RMSE={rmse:.1f} Mg/ha, R²={r2:.3f}, n={len(y)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "841256ed-b101-44a9-b7f7-4addafb1536d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patches: (0, 5, 32, 32) targets: (0,)\n"
     ]
    }
   ],
   "source": [
    "# ---- build training patches from the current clipped arrays ----\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# choose channels you want to feed the model\n",
    "CHANNELS = [red, nir, blue, ndvi, evi]  # remove ndvi/evi if you didn't compute them\n",
    "C = len(CHANNELS)\n",
    "PATCH = 32   # patch size (32x32); smaller = faster\n",
    "\n",
    "# helpers to turn world coords -> nearest pixel index\n",
    "xs = CHANNELS[0].coords[\"x\"].values\n",
    "ys = CHANNELS[0].coords[\"y\"].values  # often descending\n",
    "def nearest_index(arr, v): return int(np.argmin(np.abs(arr - v)))\n",
    "\n",
    "def extract_patch(chs, x, y, k=PATCH//2):\n",
    "    i = nearest_index(xs, x)\n",
    "    j = nearest_index(ys, y)\n",
    "    # make sure the window fits inside the raster\n",
    "    if i-k < 0 or j-k < 0 or i+k+1 > len(xs) or j+k+1 > len(ys):\n",
    "        return None\n",
    "    # stack C x H x W\n",
    "    stack = []\n",
    "    for da in chs:\n",
    "        patch = da.values[j-k:j+k+1, i-k:i+k+1]\n",
    "        stack.append(patch)\n",
    "    arr = np.stack(stack, axis=0)\n",
    "    # drop if too many NaNs (e.g., masked or cloudy)\n",
    "    if np.isnan(arr).mean() > 0.2:   # allow up to 20% NaNs\n",
    "        return None\n",
    "    # fill remaining NaNs with channel medians so training won’t crash\n",
    "    m = np.nanmedian(arr, axis=(1,2), keepdims=True)\n",
    "    arr = np.where(np.isnan(arr), m, arr)\n",
    "    return arr.astype(\"float32\")\n",
    "\n",
    "# keep GEDI points that fall inside the current raster bounds\n",
    "minx, miny, maxx, maxy = CHANNELS[0].rio.bounds()\n",
    "pts = gedi_utm.cx[minx:maxx, miny:maxy]    # same CRS as rasters\n",
    "\n",
    "X_list, y_list = [], []\n",
    "for p, row in pts.iterrows():\n",
    "    P = extract_patch(CHANNELS, row.geometry.x, row.geometry.y)\n",
    "    if P is None: \n",
    "        continue\n",
    "    X_list.append(P)\n",
    "    y_list.append(float(row[\"agbd\"]))\n",
    "\n",
    "X = np.stack(X_list, axis=0) if X_list else np.empty((0,C,PATCH,PATCH), dtype=\"float32\")\n",
    "y = np.array(y_list, dtype=\"float32\")\n",
    "\n",
    "print(\"patches:\", X.shape, \"targets:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bac09530-9e85-43bc-9b16-a819504efc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: very few patches; results will be unstable. Consider adding more granules/gedi shots.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m n_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ds) \u001b[38;5;241m-\u001b[39m n_val\n\u001b[0;32m     21\u001b[0m train_ds, val_ds \u001b[38;5;241m=\u001b[39m random_split(ds, [n_train, n_val])\n\u001b[1;32m---> 22\u001b[0m train_dl \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m val_dl   \u001b[38;5;241m=\u001b[39m DataLoader(val_ds, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m n_val\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# build a resnet18 and adapt first conv to C channels\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:376\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 376\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\sampler.py:164\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# ---- very small ResNet18 regression head (backbone frozen) ----\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import math\n",
    "\n",
    "if len(X) < 20:\n",
    "    print(\"Warning: very few patches; results will be unstable. Consider adding more granules/gedi shots.\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# tensors & split\n",
    "Xt = torch.from_numpy(X)                 # [N, C, H, W]\n",
    "yt = torch.from_numpy(y).unsqueeze(1)    # [N, 1]\n",
    "ds = TensorDataset(Xt, yt)\n",
    "\n",
    "val_frac = 0.2 if len(ds) >= 10 else 0.0\n",
    "n_val = int(len(ds)*val_frac)\n",
    "n_train = len(ds) - n_val\n",
    "train_ds, val_ds = random_split(ds, [n_train, n_val])\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_dl   = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=0) if n_val>0 else None\n",
    "\n",
    "# build a resnet18 and adapt first conv to C channels\n",
    "base = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "old_w = base.conv1.weight.data           # [64,3,7,7]\n",
    "new_w = torch.zeros((old_w.shape[0], Xt.shape[1], old_w.shape[2], old_w.shape[3]))\n",
    "mean_w = old_w.mean(dim=1, keepdim=True) # [64,1,7,7]\n",
    "new_w[:] = mean_w                        # fill with mean\n",
    "new_w[:, :min(3, Xt.shape[1])] = old_w[:, :min(3, Xt.shape[1])]\n",
    "base.conv1 = nn.Conv2d(Xt.shape[1], old_w.shape[0], kernel_size=7, stride=2, padding=3, bias=False)\n",
    "base.conv1.weight = nn.Parameter(new_w)\n",
    "\n",
    "# freeze backbone\n",
    "for p in base.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# small trainable head (replace fc)\n",
    "in_feats = base.fc.in_features\n",
    "base.fc = nn.Sequential(nn.Linear(in_feats, 128), nn.ReLU(), nn.Linear(128, 1))\n",
    "\n",
    "# only the head is trainable\n",
    "opt = torch.optim.Adam(base.fc.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "base.to(device).train()\n",
    "\n",
    "EPOCHS = 5  # short and sweet\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    base.train()\n",
    "    tr_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        pred = base(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        tr_loss += loss.item() * len(xb)\n",
    "    tr_loss /= max(1, n_train)\n",
    "\n",
    "    if val_dl:\n",
    "        base.eval()\n",
    "        with torch.no_grad():\n",
    "            y_true, y_hat = [], []\n",
    "            for xb, yb in val_dl:\n",
    "                xb = xb.to(device)\n",
    "                pred = base(xb).cpu().numpy().ravel()\n",
    "                y_hat.append(pred); y_true.append(yb.numpy().ravel())\n",
    "            y_true = np.concatenate(y_true); y_hat = np.concatenate(y_hat)\n",
    "            rmse = math.sqrt(mean_squared_error(y_true, y_hat))\n",
    "            r2   = r2_score(y_true, y_hat)\n",
    "        print(f\"epoch {epoch:02d}: train_loss={tr_loss:.3f}, val_RMSE={rmse:.1f} Mg/ha, val_R²={r2:.3f}\")\n",
    "    else:\n",
    "        print(f\"epoch {epoch:02d}: train_loss={tr_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55a9b726-ae06-44e3-91d0-5adbb8a84307",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No patches extracted. Relax the mask, shrink PATCH (e.g., 24 or 16), or add more overlapping HLS granules / GEDI files.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X)  \u001b[38;5;66;03m# number of patches\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m N \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo patches extracted. Relax the mask, shrink PATCH (e.g., 24 or 16), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor add more overlapping HLS granules / GEDI files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m     )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m N \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: only N=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m patches — training/metrics will be very unstable.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No patches extracted. Relax the mask, shrink PATCH (e.g., 24 or 16), or add more overlapping HLS granules / GEDI files."
     ]
    }
   ],
   "source": [
    "# ---- very small ResNet18 regression head (robust to tiny N) ----\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np, math\n",
    "\n",
    "N = len(X)  # number of patches\n",
    "if N == 0:\n",
    "    raise RuntimeError(\n",
    "        \"No patches extracted. Relax the mask, shrink PATCH (e.g., 24 or 16), \"\n",
    "        \"or add more overlapping HLS granules / GEDI files.\"\n",
    "    )\n",
    "if N < 4:\n",
    "    print(f\"Warning: only N={N} patches — training/metrics will be very unstable.\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "Xt = torch.from_numpy(X)                 # [N, C, H, W]\n",
    "yt = torch.from_numpy(y).unsqueeze(1)    # [N, 1]\n",
    "ds = TensorDataset(Xt, yt)\n",
    "\n",
    "# don't make train set empty\n",
    "val_frac = 0.2 if N >= 20 else 0.0\n",
    "n_val   = int(round(N * val_frac))\n",
    "n_train = N - n_val\n",
    "if n_train == 0:         # fallback: all to train, no val\n",
    "    n_train, n_val = N, 0\n",
    "\n",
    "train_ds, val_ds = random_split(ds, [n_train, n_val]) if n_val > 0 else (ds, None)\n",
    "\n",
    "bs_train = min(32, n_train) if n_train > 0 else 1\n",
    "train_dl = DataLoader(train_ds, batch_size=bs_train, shuffle=(n_train > 1), num_workers=0)\n",
    "val_dl   = DataLoader(val_ds, batch_size=min(64, n_val), shuffle=False, num_workers=0) if n_val > 0 else None\n",
    "\n",
    "# build resnet18, adapt first conv to C channels, freeze backbone\n",
    "C = Xt.shape[1]\n",
    "base = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "old_w = base.conv1.weight.data           # [64,3,7,7]\n",
    "new_w = torch.zeros((old_w.shape[0], C, old_w.shape[2], old_w.shape[3]))\n",
    "mean_w = old_w.mean(dim=1, keepdim=True)\n",
    "new_w[:] = mean_w\n",
    "new_w[:, :min(3, C)] = old_w[:, :min(3, C)]\n",
    "base.conv1 = nn.Conv2d(C, old_w.shape[0], kernel_size=7, stride=2, padding=3, bias=False)\n",
    "base.conv1.weight = nn.Parameter(new_w)\n",
    "\n",
    "for p in base.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "in_feats = base.fc.in_features\n",
    "base.fc = nn.Sequential(nn.Linear(in_feats, 128), nn.ReLU(), nn.Linear(128, 1))\n",
    "\n",
    "opt = torch.optim.Adam(base.fc.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "base.to(device)\n",
    "\n",
    "EPOCHS = 5\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    base.train()\n",
    "    tr_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        pred = base(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        tr_loss += loss.item() * len(xb)\n",
    "    tr_loss /= max(1, n_train)\n",
    "\n",
    "    if val_dl and n_val > 0:\n",
    "        base.eval()\n",
    "        yh, yt_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_dl:\n",
    "                xb = xb.to(device)\n",
    "                yh.append(base(xb).cpu().numpy().ravel())\n",
    "                yt_true.append(yb.numpy().ravel())\n",
    "        yh = np.concatenate(yh); yt_true = np.concatenate(yt_true)\n",
    "        rmse = math.sqrt(mean_squared_error(yt_true, yh))\n",
    "        r2   = r2_score(yt_true, yh) if len(yt_true) > 1 else float(\"nan\")\n",
    "        print(f\"epoch {epoch:02d}: train_loss={tr_loss:.3f}, val_RMSE={rmse:.1f} Mg/ha, val_R²={r2:.3f}\")\n",
    "    else:\n",
    "        print(f\"epoch {epoch:02d}: train_loss={tr_loss:.3f} (no val set)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b52dfde-0a5e-4edf-9769-c2b272acbf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patches: (0, 5, 32, 32) targets: (0,)\n"
     ]
    }
   ],
   "source": [
    "print(\"patches:\", X.shape, \"targets:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b7a5b6a-10c2-41eb-8c1c-c79fa61a12e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, numpy as np, pandas as pd, xarray as xr, rioxarray as rxr, rasterio\n",
    "from shapely.geometry import box\n",
    "\n",
    "# --- knobs: loosen for a demo ---\n",
    "PATCH     = 16          # smaller window -> fits inside ROI easier\n",
    "APPLY_MASK= False       # start with False; set True once you see patches\n",
    "NAN_MAX   = 0.6         # allow up to 60% NaNs inside a patch\n",
    "CHANNELS_WANTED = (\"B04\",\"B02\")  # Red, Blue (always)\n",
    "# NIR tag depends on product (B8A for S30, B05 for L30) — we’ll add it per-granule\n",
    "# Optional: you can add SWIR later once patches appear\n",
    "\n",
    "def pick(gdir, tag):\n",
    "    for f in os.listdir(gdir):\n",
    "        if re.search(rf\"\\.{tag}\\.tif$\", f): \n",
    "            return os.path.join(gdir,f)\n",
    "    return None\n",
    "\n",
    "def load_granule_channels(gdir):\n",
    "    \"\"\"Return dict of {name: DataArray} scaled to reflectance; also returns 'is_s30'.\"\"\"\n",
    "    is_s30 = (\"HLS.S30\" in gdir) or (\"HLSS30\" in gdir)\n",
    "    nir_tag = \"B8A\" if is_s30 else \"B05\"\n",
    "    # pick files\n",
    "    b04 = pick(gdir, \"B04\")\n",
    "    b02 = pick(gdir, \"B02\")\n",
    "    nir = pick(gdir, nir_tag)\n",
    "    fmsk= pick(gdir, \"Fmask\")\n",
    "    if not (b04 and b02 and nir):\n",
    "        return None, is_s30\n",
    "    # open & scale\n",
    "    red  = rxr.open_rasterio(b04).squeeze(\"band\", drop=True).astype(\"float32\") * 1e-4\n",
    "    blue = rxr.open_rasterio(b02).squeeze(\"band\", drop=True).astype(\"float32\") * 1e-4\n",
    "    nir  = rxr.open_rasterio(nir).squeeze(\"band\", drop=True).astype(\"float32\") * 1e-4\n",
    "    out = {\"red\": red, \"nir\": nir, \"blue\": blue}\n",
    "    if fmsk:\n",
    "        qa = rxr.open_rasterio(fmsk).squeeze(\"band\", drop=True)\n",
    "        out[\"qa\"] = qa\n",
    "    return out, is_s30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59a7ba36-1bbd-409f-87c8-950bdc9199a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLS.L30.T10TEK.2021096T184501.v2.0: patches=0\n",
      "HLS.L30.T10TEK.2021103T185109.v2.0: patches=0\n",
      "HLS.L30.T10TEK.2021112T184454.v2.0: patches=0\n",
      "HLS.L30.T10TEK.2021119T185101.v2.0: patches=0\n",
      "HLS.S30.T10TEK.2021092T185921.v2.0: patches=0\n",
      "HLS.S30.T10TEK.2021094T184919.v2.0: patches=0\n",
      "HLS.S30.T10TEK.2021097T185919.v2.0: patches=0\n",
      "HLS.S30.T10TEK.2021099T184911.v2.0: patches=0\n",
      "HLS.S30.T10TEK.2021102T185911.v2.0: patches=0\n",
      "HLS.S30.T10TEK.2021104T184919.v2.0: patches=0\n",
      "HLS.S30.T10TEK.2021107T185909.v2.0: patches=0\n",
      "HLS.S30.T10TEK.2021109T184911.v2.0: patches=0\n",
      "HLS.S30.T10TEK.2021112T185911.v2.0: patches=0\n",
      "HLS.S30.T10TEK.2021114T184909.v2.0: patches=0\n",
      "HLS.S30.T10TEK.2021117T185909.v2.0: patches=0\n",
      "HLS.S30.T10TEK.2021119T184921.v2.0: patches=0\n",
      "TOTAL patches: (0, 5, 16, 16) targets: (0,)\n"
     ]
    }
   ],
   "source": [
    "def extract_patches_from_granule(gdir, roi_ll, gedi_ll):\n",
    "    chans, _ = load_granule_channels(gdir)\n",
    "    if chans is None: \n",
    "        return [], []\n",
    "\n",
    "    # check overlap via B04 file bounds\n",
    "    b04 = pick(gdir, \"B04\")\n",
    "    with rasterio.open(b04) as src:\n",
    "        roi_utm = roi_ll.to_crs(src.crs)\n",
    "        if not box(*src.bounds).intersects(roi_utm.union_all()):\n",
    "            return [], []\n",
    "\n",
    "    # clip to ROI (keeps memory small)\n",
    "    geom = list(roi_utm.geometry)\n",
    "    red  = chans[\"red\"].rio.clip(geom, roi_utm.crs, drop=True, all_touched=True, from_disk=True)\n",
    "    nir  = chans[\"nir\"].rio.clip(geom, roi_utm.crs, drop=True, all_touched=True, from_disk=True)\n",
    "    blue = chans[\"blue\"].rio.clip(geom, roi_utm.crs, drop=True, all_touched=True, from_disk=True)\n",
    "    if APPLY_MASK and \"qa\" in chans:\n",
    "        qa = chans[\"qa\"].rio.clip(geom, roi_utm.crs, drop=True, all_touched=True, from_disk=True)\n",
    "        clear = (qa == 1)  # demo-clear\n",
    "        red  = red.where(clear); nir = nir.where(clear); blue = blue.where(clear)\n",
    "\n",
    "    # indices\n",
    "    ndvi = (nir - red) / (nir + red + 1e-6)\n",
    "    evi  = 2.5 * (nir - red) / (nir + 6*red - 7.5*blue + 1.0 + 1e-6)\n",
    "\n",
    "    CHANS = [red, nir, blue, ndvi, evi]  # 5 channels\n",
    "    C = len(CHANS); H = PATCH; k = H//2\n",
    "\n",
    "    xs = CHANS[0].coords[\"x\"].values\n",
    "    ys = CHANS[0].coords[\"y\"].values\n",
    "\n",
    "    def nearest_index(arr, v): \n",
    "        return int(np.argmin(np.abs(arr - v)))\n",
    "\n",
    "    # GEDI to same CRS and keep inside clip bbox\n",
    "    gedi_utm = gedi_ll.to_crs(CHANS[0].rio.crs)\n",
    "    minx, miny, maxx, maxy = CHANS[0].rio.bounds()\n",
    "    pts = gedi_utm.cx[minx:maxx, miny:maxy]\n",
    "    if len(pts) == 0:\n",
    "        return [], []\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "    for _, row in pts.iterrows():\n",
    "        i = nearest_index(xs, row.geometry.x)\n",
    "        j = nearest_index(ys, row.geometry.y)\n",
    "        if i-k < 0 or j-k < 0 or i+k+1 > len(xs) or j+k+1 > len(ys):\n",
    "            continue  # patch would go out of bounds\n",
    "        stack = []\n",
    "        for da in CHANS:\n",
    "            patch = da.values[j-k:j+k+1, i-k:i+k+1]\n",
    "            stack.append(patch)\n",
    "        arr = np.stack(stack, axis=0)  # [C, H, H]\n",
    "        # tolerate NaNs\n",
    "        if np.isnan(arr).mean() > NAN_MAX:\n",
    "            continue\n",
    "        med = np.nanmedian(arr, axis=(1,2), keepdims=True)\n",
    "        arr = np.where(np.isnan(arr), med, arr).astype(\"float32\")\n",
    "        X_list.append(arr)\n",
    "        y_list.append(float(row[\"agbd\"]))\n",
    "    return X_list, y_list\n",
    "\n",
    "# run across all granules\n",
    "X_list_all, y_list_all = [], []\n",
    "BASE = \"hls_downloads\"\n",
    "for gname in os.listdir(BASE):\n",
    "    gdir = os.path.join(BASE, gname)\n",
    "    if not os.path.isdir(gdir): \n",
    "        continue\n",
    "    Xl, yl = extract_patches_from_granule(gdir, roi_ll, gedi_in)  # gedi_in: good shots in ROI (lat/lon CRS)\n",
    "    print(f\"{gname}: patches={len(Xl)}\")\n",
    "    X_list_all.extend(Xl); y_list_all.extend(yl)\n",
    "\n",
    "X = np.stack(X_list_all, axis=0) if X_list_all else np.empty((0,5,PATCH,PATCH), dtype=\"float32\")\n",
    "y = np.array(y_list_all, dtype=\"float32\")\n",
    "print(\"TOTAL patches:\", X.shape, \"targets:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0af62cb0-98b1-4454-ba86-3356a211a3a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No patches extracted. Try PATCH=8, APPLY_MASK=False, or add more granules / GEDI files.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m N \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo patches extracted. Try PATCH=8, APPLY_MASK=False, or add more granules / GEDI files.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m N \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: only N=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m patches — results will be unstable.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No patches extracted. Try PATCH=8, APPLY_MASK=False, or add more granules / GEDI files."
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np, math\n",
    "\n",
    "N = len(X)\n",
    "if N == 0:\n",
    "    raise RuntimeError(\"No patches extracted. Try PATCH=8, APPLY_MASK=False, or add more granules / GEDI files.\")\n",
    "if N < 4:\n",
    "    print(f\"Warning: only N={N} patches — results will be unstable.\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "Xt = torch.from_numpy(X)\n",
    "yt = torch.from_numpy(y).unsqueeze(1)\n",
    "ds = TensorDataset(Xt, yt)\n",
    "\n",
    "val_frac = 0.2 if N >= 20 else 0.0\n",
    "n_val = int(round(N*val_frac)); n_train = N - n_val\n",
    "if n_train == 0: n_train, n_val = N, 0\n",
    "\n",
    "train_ds, val_ds = random_split(ds, [n_train, n_val]) if n_val>0 else (ds, None)\n",
    "train_dl = DataLoader(train_ds, batch_size=min(32, n_train), shuffle=(n_train>1), num_workers=0)\n",
    "val_dl   = DataLoader(val_ds, batch_size=min(64, n_val), shuffle=False, num_workers=0) if n_val>0 else None\n",
    "\n",
    "C = Xt.shape[1]\n",
    "base = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "old_w = base.conv1.weight.data\n",
    "new_w = torch.zeros((old_w.shape[0], C, old_w.shape[2], old_w.shape[3]))\n",
    "mean_w = old_w.mean(dim=1, keepdim=True)\n",
    "new_w[:] = mean_w\n",
    "new_w[:, :min(3, C)] = old_w[:, :min(3, C)]\n",
    "base.conv1 = nn.Conv2d(C, old_w.shape[0], kernel_size=7, stride=2, padding=3, bias=False)\n",
    "base.conv1.weight = nn.Parameter(new_w)\n",
    "\n",
    "for p in base.parameters(): p.requires_grad = False\n",
    "in_feats = base.fc.in_features\n",
    "base.fc = nn.Sequential(nn.Linear(in_feats, 128), nn.ReLU(), nn.Linear(128, 1))\n",
    "\n",
    "opt = torch.optim.Adam(base.fc.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "base.to(device)\n",
    "\n",
    "EPOCHS = 5\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    base.train(); tr_loss = 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad(); pred = base(xb); loss = loss_fn(pred, yb)\n",
    "        loss.backward(); opt.step()\n",
    "        tr_loss += loss.item()*len(xb)\n",
    "    tr_loss /= max(1, n_train)\n",
    "\n",
    "    if val_dl:\n",
    "        base.eval(); yh, yt_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_dl:\n",
    "                xb = xb.to(device)\n",
    "                yh.append(base(xb).cpu().numpy().ravel())\n",
    "                yt_true.append(yb.numpy().ravel())\n",
    "        yh = np.concatenate(yh); yt_true = np.concatenate(yt_true)\n",
    "        rmse = math.sqrt(mean_squared_error(yt_true, yh))\n",
    "        r2 = r2_score(yt_true, yh) if len(yt_true) > 1 else float(\"nan\")\n",
    "        print(f\"epoch {ep:02d}: train_loss={tr_loss:.3f}, val_RMSE={rmse:.1f}, val_R²={r2:.3f}\")\n",
    "    else:\n",
    "        print(f\"epoch {ep:02d}: train_loss={tr_loss:.3f} (no val)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4baaa6-99aa-4875-93a9-1e0367a7c6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
